{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 京大BERTファインチューニング\n",
    "[京大BERT](https://nlp.ist.i.kyoto-u.ac.jp/?ku_bert_japanese)をベースにして、[ストックマーク株式会社が公開しているner-wikipedia-dataset](https://github.com/stockmarkteam/ner-wikipedia-dataset)を使って固有表現抽出タスク向けにファインチューニングを行う例です  \n",
    "PyTorch+transformersです(not Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "学習に必要なものを用意します\n",
    "主に必要になるもの\n",
    "- [京大BERTモデル](https://nlp.ist.i.kyoto-u.ac.jp/?ku_bert_japanese)\n",
    "- [Juman++](https://nlp.ist.i.kyoto-u.ac.jp/?JUMAN%2B%2B)\n",
    "- [pyknp](https://github.com/ku-nlp/pyknp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://lotus.kuee.kyoto-u.ac.jp/nl-resource/JapaneseBertPretrainedModel/Japanese_L-24_H-1024_A-16_E-30_BPE_WWM_transformers.zip&name=Japanese_L-24_H-1024_A-16_E-30_BPE_WWM_transformers.zip\" -O bert.zip\n",
    "!unzip bert.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kyoto\n",
    "!mv Japanese_L-24_H-1024_A-16_E-30_BPE_WWM_transformers kyoto/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※Juman++のインストールは大きめのインスタンスでないと時間がかかるorフリーズするかもしれません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc2/jumanpp-2.0.0-rc2.tar.xz\"\n",
    "!tar xvf jumanpp-2.0.0-rc2.tar.xz\n",
    "!apt-get update -y\n",
    "!apt-get install -y cmake gcc build-essential\n",
    "%cd jumanpp-2.0.0-rc2\n",
    "!mkdir bld\n",
    "%cd bld\n",
    "!cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
    "!make install -j\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers[\"ja\"] numpy noyaki sklearn pyknp\n",
    "!pip install -U jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs\n",
    "!mkdir ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認\n",
    "Juman++が動いていることを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"こんにちは\" | jumanpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データのダウンロード\n",
    "今回は冒頭でも述べたとおり[ストックマーク株式会社が公開しているner-wikipedia-dataset](https://github.com/stockmarkteam/ner-wikipedia-dataset)を利用させていただきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/stockmarkteam/ner-wikipedia-dataset/raw/main/ner.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの確認\n",
    "ダウンロードしてきた`ner.json`がどのようになっているか軽く確認してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -15 ner.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "実際に学習をしてみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForTokenClassification, BertTokenizer, BertConfig,\n",
    "    TrainingArguments, Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from pyknp import Juman\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import noyaki\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数を定義しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(path: str) -> list:\n",
    "    jumanpp = Juman()\n",
    "    json_dict = json.load(open(path, \"r\"))\n",
    "    features = []\n",
    "    for unit in json_dict:\n",
    "        result = jumanpp.analysis(unit[\"text\"])\n",
    "        tokenized_text = [mrph.midasi for mrph in result.mrph_list()]\n",
    "        spans = []\n",
    "        for entity in unit[\"entities\"]:\n",
    "            span_list = []\n",
    "            span_list.extend(entity[\"span\"])\n",
    "            span_list.append(entity[\"type\"])\n",
    "            spans.append(span_list)\n",
    "        label = noyaki.convert(tokenized_text, spans)\n",
    "        features.append({\"x\": tokenized_text, \"y\": label})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_vocab(features: list) -> tuple:\n",
    "    labels = [f[\"y\"] for f in features]\n",
    "    unique_labels = list(set(sum(labels, [])))\n",
    "    label2id = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label2id[label] = i\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features: list) -> dict:\n",
    "    x = [f[\"x\"] for f in features]\n",
    "    y = [f[\"y\"] for f in features]\n",
    "    inputs = tokenizer(x, return_tensors=None, padding='max_length', truncation=True, max_length=64, is_split_into_words=True)\n",
    "    input_labels = []\n",
    "    for labels in y:\n",
    "        pad_list = [-100] * 64\n",
    "        for i, label in enumerate(labels):\n",
    "            pad_list.insert(i, label2id[label])\n",
    "        input_labels.append(pad_list[:64])\n",
    "    inputs['labels'] = input_labels\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in inputs.items()}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数を定義しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"./outputs\"\n",
    "ckpt_dir = \"./ckpt\"\n",
    "training_data_directory = \"./\"\n",
    "base_model_directory = \"./kyoto/bert\"\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 3e-5\n",
    "save_freq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(base_model_directory, tokenize_chinese_chars=False, do_lower_case=False)\n",
    "features = load_from_json(os.path.join(training_data_directory, \"ner.json\"))\n",
    "label2id, id2label = create_label_vocab(features)\n",
    "\n",
    "train_data, val_data = train_test_split(features, test_size=0.2, random_state=123)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`features`の中身を確認してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`label2id`と`id2label`の中身を確認してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの用意をします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(base_model_directory, label2id=label2id, id2label=id2label)\n",
    "model = BertForTokenClassification.from_pretrained(base_model_directory, config=config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の設定をつくります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir=ckpt_dir,\n",
    "                         do_train=True,\n",
    "                         do_eval=True,\n",
    "                         do_predict=True,\n",
    "                         per_device_train_batch_size=batch_size,\n",
    "                         per_device_eval_batch_size=batch_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         num_train_epochs=epochs,\n",
    "                         evaluation_strategy=\"steps\",\n",
    "                         eval_steps=save_freq,\n",
    "                         save_strategy=\"steps\",\n",
    "                         save_steps=save_freq,\n",
    "                         load_best_model_at_end=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainerをつくります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_data,\n",
    "                  eval_dataset=val_data,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストしてみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, metrics = trainer.predict(test_data, metric_key_prefix=\"test\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n",
    "できあがったモデルを使って推論を行ってみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"田中さんはhogehoge株式会社の社員です\"\n",
    "model = BertForTokenClassification.from_pretrained(\"outputs\")\n",
    "\n",
    "jumanpp = Juman()\n",
    "result = jumanpp.analysis(text)\n",
    "tokenized_text = [mrph.midasi for mrph in result.mrph_list()]\n",
    "inputs = tokenizer(tokenized_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64, is_split_into_words=True)\n",
    "pred = model(**inputs).logits[0]\n",
    "pred = np.argmax(pred.detach().numpy(), axis=-1)\n",
    "labels = []\n",
    "for i, label in enumerate(pred):\n",
    "    if i + 1 > len(tokenized_text):\n",
    "        continue\n",
    "    labels.append(model.config.id2label[label])\n",
    "    print(f\"{tokenized_text[i]}: {model.config.id2label[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 東北大BERTをベースにファインチューニングで固有表現抽出用モデルを作成する\n",
    "huggingfaceで公開されている東北大BERTこと `cl-tohoku/bert-base-japanese-whole-word-masking` をベースに、ファインチューニングをして固有表現抽出タスク用のモデルを作成します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "必要なライブラリをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install transformers[\"ja\"] numpy noyaki sklearn seqeval\n",
    "!pip3 install -U jupyter ipywidgets\n",
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データのダウンロード\n",
    "今回は[ストックマーク株式会社が公開しているner-wikipedia-dataset](https://github.com/stockmarkteam/ner-wikipedia-dataset)を利用させていただきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://github.com/stockmarkteam/ner-wikipedia-dataset/raw/main/ner.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードした学習データを確認してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -15 ner.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "実際に学習を行っていきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"./dest\"\n",
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizerの準備\n",
    "Tokenizerを用意します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、NEologdを使いたい場合など、TokenizerのMeCabにオプションを渡したい場合は[こちら](https://qiita.com/ken11_/items/fd20e69103bb0ce698af)を参考にしてください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの前処理\n",
    "先ほどダウンロードしてきた学習データを、学習に使えるように前処理していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noyaki\n",
    "import json\n",
    "\n",
    "def load_from_json(path: str) -> list:\n",
    "    json_dict = json.load(open(path, \"r\"))\n",
    "    features = []\n",
    "    for unit in json_dict:\n",
    "        tokenized_text = tokenizer.tokenize(unit[\"text\"])\n",
    "        spans = []\n",
    "        for entity in unit[\"entities\"]:\n",
    "            span_list = []\n",
    "            span_list.extend(entity[\"span\"])\n",
    "            span_list.append(entity[\"type\"])\n",
    "            spans.append(span_list)\n",
    "        label = noyaki.convert(tokenized_text, spans, subword=\"##\")\n",
    "        features.append({\"x\": tokenized_text, \"y\": label})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_from_json(\"./ner.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "featuresの中身を確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データを `train`, `valid`, `test` に分割します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(features, test_size=0.2, random_state=123)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベル辞書の作成\n",
    "ラベルの辞書を作成します  \n",
    "これはあとでmodelのconfigに渡す情報となります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_vocab(features: list) -> tuple:\n",
    "    labels = [f[\"y\"] for f in features]\n",
    "    unique_labels = list(set(sum(labels, [])))\n",
    "    label2id = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label2id[label] = i\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = create_label_vocab(features)\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの準備\n",
    "ベースモデルを用意します  \n",
    "ここで先ほどの `label2id`, `id2label` を渡してあげることで、推論時のラベル復元が楽になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "model = BertForTokenClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerの準備\n",
    "TrainingArgumentsを設定し、Trainerを作成していきます  \n",
    "Trainerにはdata_collatorを渡してあげる必要があるので、data_collatorも作成します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_collatorは[transformersにすでにあるもの](https://huggingface.co/docs/transformers/main_classes/data_collator)を利用することもできますが、ここでは自前で定義していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    x = [f[\"x\"] for f in features]\n",
    "    y = [f[\"y\"] for f in features]\n",
    "    inputs = tokenizer(x, return_tensors=None, padding='max_length', truncation=True, max_length=64, is_split_into_words=True)\n",
    "    input_labels = []\n",
    "    for labels in y:\n",
    "        pad_list = [-100] * 64\n",
    "        for i, label in enumerate(labels):\n",
    "            pad_list.insert(i, label2id[label])\n",
    "        input_labels.append(pad_list[:64])\n",
    "    inputs['labels'] = input_labels\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in inputs.items()}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハイパーパラメータなどを定義しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"./ckpt\"\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "learning_rate = 3e-5\n",
    "save_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(output_dir=ckpt_dir,\n",
    "                         do_train=True,\n",
    "                         do_eval=True,\n",
    "                         do_predict=True,\n",
    "                         per_device_train_batch_size=batch_size,\n",
    "                         per_device_eval_batch_size=batch_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         num_train_epochs=epochs,\n",
    "                         evaluation_strategy=\"steps\",\n",
    "                         eval_steps=save_freq,\n",
    "                         save_strategy=\"steps\",\n",
    "                         save_steps=save_freq,\n",
    "                         load_best_model_at_end=True,\n",
    "                         remove_unused_columns=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_data,\n",
    "                  eval_dataset=val_data,\n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できあがったモデルをテストします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, metrics = trainer.predict(test_data, metric_key_prefix=\"test\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルをsaveします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの検証\n",
    "[seqeval](https://github.com/chakki-works/seqeval)を使って実際のモデル精度を検証していきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論用の関数を定義\n",
    "学習したモデルを使って推論をするための関数を定義します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inference_model = BertForTokenClassification.from_pretrained(model_output_dir)\n",
    "def inference(tokenized_text: list) -> list:\n",
    "    inputs = tokenizer(tokenized_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64, is_split_into_words=True)\n",
    "    pred = inference_model(**inputs).logits[0]\n",
    "    pred = np.argmax(pred.detach().numpy(), axis=-1)\n",
    "    labels = []\n",
    "    for i, label in enumerate(pred):\n",
    "        if i + 1 > len(tokenized_text):\n",
    "            continue\n",
    "        labels.append(inference_model.config.id2label[label])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解データを用意します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for unit in test_data:\n",
    "    # 今回はmax_lengthを64にしているので正解データも切り詰めておく\n",
    "    y_true.append(unit[\"y\"][:64])\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に推論結果も用意します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for unit in test_data:\n",
    "    y_pred.append(inference(unit[\"x\"]))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seqevalのclassification_reportを実行\n",
    "seqevalのclassification_reportを使って検証します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import BILOU\n",
    "\n",
    "print(classification_report(y_true, y_pred, mode='strict', scheme=BILOU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seqevalのstrictモードは厳密なので精度は低くなりがちです  \n",
    "BILUOではstrictモードしかサポートされていないため、適宜BILUOをBIOに変換して使用するなど、タスクに合った精度検証を行ってください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n",
    "最後に、通常の推論用コードを紹介します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text: str):\n",
    "    model = BertForTokenClassification.from_pretrained(model_output_dir)\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    inputs = tokenizer(tokenized_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64, is_split_into_words=True)\n",
    "    pred = model(**inputs).logits[0]\n",
    "    pred = np.argmax(pred.detach().numpy(), axis=-1)\n",
    "    labels = []\n",
    "    for i, label in enumerate(pred):\n",
    "        if i + 1 > len(tokenized_text):\n",
    "            continue\n",
    "        labels.append(inference_model.config.id2label[label])\n",
    "    print(tokenized_text)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(inference(\"田中さんの会社の社長は鈴木さんです\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
